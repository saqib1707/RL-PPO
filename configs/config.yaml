# training hparams
num_epochs: 1000
num_train_steps: 10000
policy_update_interval: 1000
batch_size: 64
gamma: 0.99
clip_thresh: 0.2
action_std_init: 0.5
action_std_decay_rate: 0.05
min_action_std: 0.1
entropy_coef: 0.01
value_loss_coef: 0.5
max_grad_norm: 0.5
mode: "train"

# network hparams
hidden_dim: 64
activation: "tanh"
lr_actor: 0.0003
lr_critic: 0.001

# env hparams
env_name: "CartPole-v1"
continuous_action_space: false
random_seed: 42
max_eps_steps: 1000

# logging hparams
wandb_project: "ppo-experiments"
# log_interval: ${max_eps_steps * 5}
# save_interval: ${max_eps_steps * 10}
log_interval: 1000
save_interval: 1000
log_dir: "../logs/"
ckpt_dir: "../checkpoints/"